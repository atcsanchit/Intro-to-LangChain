{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o1ujdXl7WQ_2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core langgraph>0.2.27"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-mistralai"
      ],
      "metadata": {
        "id": "YsNm35RmbIQS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "model = ChatMistralAI(model=\"mistral-large-latest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GQp7fAJbSR0",
        "outputId": "9e044c9f-aa3c-4643-997f-73c744184db2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model.invoke([HumanMessage(content=\"Hi! I'm Sanchit\")]).content\n",
        "# model.invoke([HumanMessage(content=\"Hi! I'm Sanchit\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7ojkuNHSboNa",
        "outputId": "e014ed4a-789c-40d7-94d6-b7b0d97546f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Sanchit! Nice to meet you. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the model have no concept of state/memory/chat history\n",
        "# model.invoke([HumanMessage(content=\"What's my name?\")])\n",
        "model.invoke([HumanMessage(content=\"What's my name?\")]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "g0G9OnpNb92l",
        "outputId": "db61ac1c-2c6f-4aab-aca7-a4da6c4ddcac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm afraid I don't know your name. You haven't introduced yourself to me yet. Would you like to tell me your name?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "# model.invoke([HumanMessage(content=\"Hi! I'm Sanchit\"),\n",
        "#               AIMessage(content=\"Hello Sanchit! Nice to meet you. How can I assist you today?\"),\n",
        "#               HumanMessage(content=\"What's my name?\")\n",
        "#               ])\n",
        "model.invoke([HumanMessage(content=\"Hi! I'm Sanchit\"),\n",
        "              AIMessage(content=\"Hello Sanchit! Nice to meet you. How can I assist you today?\"),\n",
        "              HumanMessage(content=\"What's my name?\")\n",
        "              ]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HTpuAce1cR-W",
        "outputId": "efbdad3d-d9ef-43b2-a361-b0e44b46c116"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on your introduction, your name is Sanchit.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Message Persistence\n",
        "\n",
        "\n",
        "*   Message Persistence in chatbot development refers to the capability to store and retrieve user conversations, interactions, and messages across multiple sessions. This allows the chatbot to maintain context, remember past interactions, and provide a more seamless and personalized experience to the user.\n",
        "*   In simpler terms, it ensures that the history of conversations isn't lost once the chat session ends or when the user returns at a later time.\n",
        "*   For the above functionality, `LangGraph` will be use to store the state of model and conversation in a node structure which will be use to get the status and state of the model and previous context of the conversation."
      ],
      "metadata": {
        "id": "jd57_WGwo5qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "rb2fZGDfdPZc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "66ZnUoVNgfS9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm sanchit.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIMYTIAzgtqb",
        "outputId": "ef7d1c7b-0aec-40c0-ff8f-be15df0effec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Sanchit! Nice to meet you. How can I assist you today? Let me know if you have any questions or topics you'd like to discuss. 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name? and also a breif introduction to LangChain?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Czcz7Cg1n2",
        "outputId": "4e64ddec-844b-44fc-ca25-42a77ca8e938"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Sanchit! Your name is Sanchit.\n",
            "\n",
            "As for LangChain, it is an open-source framework designed to facilitate the development of applications that leverage large language models (LLMs). LangChain provides tools and abstractions that make it easier to build complex applications by managing the interactions between different components, such as LLMs, data sources, and user interfaces.\n",
            "\n",
            "Some key features of LangChain include:\n",
            "\n",
            "1. **Chains**: These are sequences of components that can be linked together to perform complex tasks. Chains can be used to break down a problem into smaller, manageable steps.\n",
            "\n",
            "2. **Prompts**: LangChain provides tools for managing and organizing prompts, which are the inputs given to the language models to generate outputs.\n",
            "\n",
            "3. **Memory**: LangChain supports various types of memory to keep track of the state and context across multiple interactions, which is crucial for building conversational applications.\n",
            "\n",
            "4. **Agents**: These are more advanced components that can make decisions based on the current context and previous interactions, allowing for more dynamic and adaptive applications.\n",
            "\n",
            "5. **Integration**: LangChain can integrate with various LLMs and data sources, making it a flexible and powerful tool for building a wide range of applications.\n",
            "\n",
            "By using LangChain, developers can focus more on the specific logic and functionality of their applications rather than dealing with the low-level details of interacting with language models and managing data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates\n",
        "\n",
        "*   Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
        "*   To add in a system message, we will create a `ChatPromptTemplate`. We will utilize `MessagesPlaceholder` to pass all the messages in.\n",
        "\n"
      ],
      "metadata": {
        "id": "vA4oVWvip7S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "P24fF8uPqT_x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    chain = prompt | model\n",
        "    response = chain.invoke(state)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "ctWWwGsaqc8X"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
        "query = \"Hi! I'm Sanchit.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdIWEHhmqfVV",
        "outputId": "06ee04c9-d1c1-4e41-f52d-3906e1f89cc6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ahoy there, Sanchit! It be a pleasure to make yer acquaintance. What be ye needin' today, matey?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDkyFSriqktr",
        "outputId": "c147f19f-acaf-472a-993e-c6ba9d0a1c26"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Arr matey, ye already told me that. Yer name be Sanchit, ain't it?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now, if we use placeholders in the system prompt then it will change call_model function"
      ],
      "metadata": {
        "id": "bm4tb3vFqq5I"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language} and in {mood} mood.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Lzfw38uIuHLC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    language: str\n",
        "    mood: str\n",
        "\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "def call_model(state: State):\n",
        "    chain = prompt | model\n",
        "    response = chain.invoke(state)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "857kduHsuKDr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
        "query = \"Hi! I'm Sanchit.\"\n",
        "language = \"Hindi\"\n",
        "mood = \"Sad\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language, \"mood\": mood},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U07s8DykuLgS",
        "outputId": "baf89eb7-910e-4b8b-f796-03a263257c71"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "नमस्ते, संचित। मैं उदास हूँ, लेकिन आपके साथ बात करने के लिए यहाँ हूँ। कृपया बताइए, मैं आपकी क्या मदद कर सकता हूँ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "# language = \"Sanskrit\"\n",
        "mood = \"Happy\"\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"mood\":mood},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK6uPDpGuSw_",
        "outputId": "de3bd330-e4b5-4088-e371-4b668b4e2496"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "नमस्ते, संचित! आपका नाम संचित है। क्या मैं आपकी कोई और मदद कर सकता हूँ? मुझे आपके साथ बात करना अच्छा लग रहा है!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Conversation History\n"
      ],
      "metadata": {
        "id": "4kxUZ4qgzB00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages"
      ],
      "metadata": {
        "id": "rkis2ajOzBj2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer = trim_messages(\n",
        "    max_tokens=65,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")"
      ],
      "metadata": {
        "id": "fVKeTGpLzRFD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    language: str\n",
        "    mood: str\n",
        "\n",
        "def call_model(state: State):\n",
        "    chain = prompt | model\n",
        "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "    response = chain.invoke(\n",
        "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
        "    )\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "7R96UA5sug2A"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language} and in {{mood}} mood.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "nqhdNeLgzm9Q"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc88906\"}}\n",
        "query = \"Hi! I'm Sanchit.\"\n",
        "language = \"Hindi\"\n",
        "mood = \"Sad\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language, \"mood\": mood},\n",
        "    config,\n",
        ")\n",
        "\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKjFjWZEzNMD",
        "outputId": "07332b83-bb69-44de-b212-b33fe0cc7c7b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Namaste Sanchit! 😊 Aap kaise hai? (How are you?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hGu0vZG-0dEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}